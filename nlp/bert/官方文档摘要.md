# 

We are releasing the following:

- TensorFlow code for the BERT model architecture (**which is mostly a standard Transformer architecture**).
- Pre-trained checkpoints for both the lowercase and cased version of BERT-Base and BERT-Large from the paper.
- TensorFlow code for push-button replication of the most important fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.

All of the code in this repository works **out-of-the-box** with CPU, GPU, and Cloud TPU

## Pre-trained models

- **Uncased** means that the text **has been lowercased before** WordPiece tokenization, e.g., John Smith becomes john smith.

- **Cased** means that the true case and accent markers are preserved. Typically, the Uncased model is **better** unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging).

**When using a cased model, make sure to pass --do_lower=False to the training scripts. (Or pass do_lower_case=False directly to FullTokenizer if you're using your own script.)**

Each .zip file contains three items:

- A TensorFlow checkpoint (bert_model.ckpt) containing the pre-trained weights (which is actually **3** files).
- A vocab file (vocab.txt) to **map WordPiece to word id**.
- A config file (bert_config.json) which specifies the hyperparameters of the model.

## Fine-tuning with **BERT**
